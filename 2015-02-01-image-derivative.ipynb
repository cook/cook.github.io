{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to show how to deduce the image derivatives from various assumption and constrains.\n",
    "\n",
    "思路：\n",
    "* 用不同的算法优化，例如线性规划\n",
    "* 用不同的模型描述，例如NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A one dimension image can be viewed as a function of $x$, $$\\begin{equation} Image = f(x)\\label{eq:image} \\end{equation}$$\n",
    ", and after sampled to a digital image, $$\\begin{equation} Digital Image = g(n) \\label{eq:digit_image} \\end{equation}$$, where $f(n) = g(n)$ and $n \\in \\{0, 1, \\cdots, N-1\\}$.\n",
    "\n",
    "Now the question is how to calculate the derivative $f'(n)$. At first it's easy to find out, in most cases, the precise can not be get because \n",
    "\n",
    "1. There is not an analytical expression of $f(x)$;\n",
    "2. We only know the value of $g(0), g(1), \\cdots, g(N-1)$.\n",
    "\n",
    "So to resolve the problem, what need to do is to find out a good estimation of $f'(n)$, $\\hat{f'}(n)$.\n",
    "\n",
    "For convenience, I still use $g(x)$ instead of $g(n)$ in the following discussion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From statistical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prewitt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roberts Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Underdestermined Coefficient with Taylor Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume within a local region of $x_0$, lets say $\\Psi_{x_0}$, $$\\begin{equation} f(x) = h(x) + \\epsilon \\end{equation}$$, where $$\\begin{equation} h(x) = h(x_0) + (x - x_0)f'(x_0) \\end{equation}$$. This is a first order Taylor expansion.\n",
    "\n",
    "According to the Taylor theorem, $\\epsilon = \\frac{(x - x_0)^2}{2!}f'(\\xi)$, where $\\xi \\in [x_0, x+x_0]$.\n",
    "\n",
    "Let's suppose $x$ takes on values of $\\Psi = \\{-1, 0, 1\\}$. Then I get \n",
    "$$\\begin{align} \n",
    "\\epsilon(1) &= f(1) - f(0) - f'(0)\\\\\n",
    "\\epsilon(0) &= 0\\\\\n",
    "\\epsilon(-1) &= f(-1) - f(0) + f'(0)\n",
    "\\end{align}$$\n",
    "If I assume $$\\begin{equation}\\sum_{x \\in \\Psi}\\epsilon(x) = 0\\end{equation}$$, then I get $$f'(0) = \\frac{f(1) - f(-1)}{2}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Least Mean Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume within a local region of $x_0$, according to the Taylor theorem, set $\\Psi_{x_0}$, $$\\begin{equation} f(x) \\approx h(x) = wx + b\\end{equation},$$where $w$ is the $\\hat{f'}(x_0)$. \n",
    "\n",
    "Let's suppose $x$ takes on values of $\\Psi = \\{-1, 0, 1\\}$. Then I get the optimal $$w^*=\\frac{1}{2}argmin_w E(w) = argmin_w \\sum_{x \\in \\Psi}(wx + b - g(x))^2,$$get $$\\frac{\\partial{E(w)}}{\\partial{w}} = 0 \\Rightarrow w = \\frac{f(1) - f(-1)}{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a quadratic polynomial to simulate a local curve, that is $$\\begin{equation} f(x) \\approx h(x) = ax^2 + bx + c,\\end{equation}$$where, without loss of generality, $x \\in \\{-1, 0, 1\\}$. Then $h''(x) = 2a$ can be taken as a approximation of $f''(x)$.\n",
    "\n",
    "Set $\\mathbf{w} = [a, b, c]^T$, $\\mathbf{x} = [x^2, x, 1]^T$, we want to find out\n",
    "$$\\mathbf{w}^* = argmin_{\\mathbf{w}} E(\\mathbf{w}),$$where\n",
    "$$E(\\mathbf{w}) = \\sum_x(\\mathbf{x}^T\\mathbf{w} - f(x))^2.$$\n",
    "So we my take derivatives and set it to zero,\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{w}}} &= 2 \\sum_x \\mathbf{x}(\\mathbf{x}^T\\mathbf{w} - f(x)) \\\\\n",
    "&= 2 ((\\sum_x \\mathbf{x}\\mathbf{x}^T)\\mathbf{w} - \\sum_x \\mathbf{x}f(x)) \\\\\n",
    "&= 0\n",
    "\\end{align}.$$\n",
    "Because, \n",
    "$$\\begin{align} \\sum_x \\mathbf{x}\\mathbf{x}^T &= \\left[\\matrix{\\sum_xx^4 & \\sum_xx^3 & \\sum_xx^2\\\\ \\sum_xx^3 & \\sum_xx^2 & \\sum_xx\\\\ \\sum_xx^2 & \\sum_xx & \\sum_x1}\\right] \\\\\n",
    "&= \\left[\\matrix{2&0&2\\\\ 0&2&0\\\\ 2&0&3}\\right]\\end{align},$$\n",
    "so we get a set of equations,\n",
    "$$\\left[\\matrix{2&0&2\\\\ 0&2&0\\\\ 2&0&3}\\right] \\cdot \\left[\\matrix{a\\\\ b\\\\ c}\\right] = \\left[\\matrix{\\sum_xx^2f(x)\\\\ \\sum_xxf(x)\\\\ \\sum_xf(x)}\\right].$$\n",
    "Resolve these equations, we get $$\\begin{align}f'(0) &\\approx h'(0) = b = \\frac{f(1)-f(-1)}{2}\\\\\n",
    "f''(0) &\\approx h''(0) = 2a = f(-1) - 2f(0) + f(1) = \\left[\\matrix{1&-2&1}\\right] \\cdot \\left[\\matrix{f(-1)\\\\ f(0)\\\\ f(1)}\\right]\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Lagrange Interpolating Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $n+1$ data, $(x_0,y_0),\\cdots,(x_n,y_n)$，where all $x_i$ are different, a Lagrange's formual of order $\\le n$ is defined as\n",
    "$$P_n(x)=y_0L_0(x)+y_1L_1(x)+\\cdots+y_nL_n(x),$$\n",
    "where $L_i(x)$, the Lagrange interpolation basis functions, is defined as\n",
    "$$L_i(x)=\\frac{(x-x_0) \\cdots (x-x_{i-1})(x-x_{i+1}) \\cdots (x-x_n)}{(x_i-x_0) \\cdots (x_i-x_{i-1})(x_i-x_{i+1}) \\cdots (x_i-x_n)}.$$\n",
    "\n",
    "Now given $(x_{-1}=-1, f(-1)), (x_0=0, f(0)), (x_1=1, f(1))$, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "P_2(x) &= \\frac{(x - x_0)(x - x_1)}{(x_{-1}-x_0)(x_{-1}-x_1)}f(x_{-1}) + \\frac{(x - x_{-1})(x - x_1)}{(x_0-x_{-1})(x_0-x_1)}f(x_0) + \\frac{(x - x_{-1})(x - x_0)}{(x_1-x_{-1})(x_1-x_0)}f(x_1) \\\\\n",
    "&= \\frac{x(x - 1)}{2}f(-1) - (x + 1)(x - 1)f(0) + \\frac{x(x + 1)}{2}f(1)\n",
    "\\end{align}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\begin{align}\n",
    "P_2'(x) &= \\frac{2x-1}{2}f(-1) - 2xf(0) + \\frac{2x+1}{2}f(1)\\\\\n",
    "P_2'(0) &= \\frac{f(1)-f(-1)}{2} \\\\\n",
    "P_2''(x) &= P_2''(0) = f(-1) - 2f(0) + f(1) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irwin Sobel, History and Definition of Sobel Operator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
